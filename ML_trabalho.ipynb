{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "u1Rg7z1-Y3kx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c2411dc-81c5-4d67-8349-7647dfd6f500",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scikit-posthocs\n",
            "  Downloading scikit_posthocs-0.11.2-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from scikit-posthocs) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from scikit-posthocs) (1.13.1)\n",
            "Requirement already satisfied: statsmodels in /usr/local/lib/python3.11/dist-packages (from scikit-posthocs) (0.14.4)\n",
            "Requirement already satisfied: pandas>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from scikit-posthocs) (2.2.2)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (from scikit-posthocs) (0.13.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from scikit-posthocs) (3.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.20.0->scikit-posthocs) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.20.0->scikit-posthocs) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.20.0->scikit-posthocs) (2025.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->scikit-posthocs) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->scikit-posthocs) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->scikit-posthocs) (4.55.8)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->scikit-posthocs) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->scikit-posthocs) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->scikit-posthocs) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->scikit-posthocs) (3.2.1)\n",
            "Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.11/dist-packages (from statsmodels->scikit-posthocs) (1.0.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=0.20.0->scikit-posthocs) (1.17.0)\n",
            "Downloading scikit_posthocs-0.11.2-py3-none-any.whl (33 kB)\n",
            "Installing collected packages: scikit-posthocs\n",
            "Successfully installed scikit-posthocs-0.11.2\n"
          ]
        }
      ],
      "source": [
        "!pip install scikit-posthocs\n",
        "import cv2\n",
        "import os\n",
        "from skimage.feature import hog\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.preprocessing import image\n",
        "from keras.applications.vgg16 import preprocess_input\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from scipy.stats import friedmanchisquare\n",
        "import scikit_posthocs as sp\n",
        "import scipy.stats as stats\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Caminho do diretório\n",
        "dir_path = \"/content/basset_hound\"\n",
        "dir_path2 = \"/content/york_terrier\"\n",
        "dir_path3 = \"/content/abyssinian\"\n",
        "dir_path4 = \"/content/egyptian\"\n",
        "\n",
        "# Cria o diretório\n",
        "os.makedirs(dir_path, exist_ok=True)\n",
        "os.makedirs(dir_path2, exist_ok=True)\n",
        "os.makedirs(dir_path3, exist_ok=True)\n",
        "os.makedirs(dir_path4, exist_ok=True)\n"
      ],
      "metadata": {
        "id": "jVBp3qK8siqo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lista de diretórios\n",
        "diretorios = ['/content/basset_hound', '/content/york_terrier', '/content/abyssinian', '/content/egyptian']\n",
        "\n",
        "# Criar DataFrames vazios\n",
        "df1 = pd.DataFrame()\n",
        "df2 = pd.DataFrame()\n",
        "\n",
        "# Para cada diretório na lista de diretórios\n",
        "for diretorio in diretorios:\n",
        "    # Para cada arquivo no diretório\n",
        "    for nome_arquivo in os.listdir(diretorio):\n",
        "        if nome_arquivo.endswith(('.png','.jpg', '.jpeg')):\n",
        "            caminho_arquivo = os.path.join(diretorio, nome_arquivo)\n",
        "            imagem = cv2.imread(caminho_arquivo)\n",
        "            if imagem is not None:\n",
        "                # Redimensionar a imagem para 128x128\n",
        "                imagem_redimensionada = cv2.resize(imagem, (128, 128))\n",
        "\n",
        "                # Aplicar HOG com pixels_per_cell=(16,16)\n",
        "                fd1_temp, hog_image1 = hog(imagem_redimensionada, orientations=9, pixels_per_cell=(16,16),\n",
        "                                       cells_per_block=(2, 2), visualize=True, multichannel=True)\n",
        "\n",
        "                # Aplicar HOG com pixels_per_cell=(20,20)\n",
        "                fd2_temp, hog_image2 = hog(imagem_redimensionada, orientations=9, pixels_per_cell=(20,20),\n",
        "                                       cells_per_block=(2, 2), visualize=True, multichannel=True)\n",
        "\n",
        "                # Adicionar os descritores HOG a um DataFrame pandas temporário\n",
        "                df1_temp = pd.DataFrame(fd1_temp).transpose()\n",
        "                df2_temp = pd.DataFrame(fd2_temp).transpose()\n",
        "\n",
        "                # Adicionar uma coluna 'label' para indicar se a imagem é de um cachorro ou gato\n",
        "                df1_temp['label'] = 'cachorro' if 'basset_hound' in diretorio or 'york_terrier' in diretorio else 'gato'\n",
        "                df2_temp['label'] = 'cachorro' if 'basset_hound' in diretorio or 'york_terrier' in diretorio else 'gato'\n",
        "\n",
        "                # Anexar os DataFrames temporários aos DataFrames principais\n",
        "                df1 = df1.append(df1_temp)\n",
        "                df2 = df2.append(df2_temp)\n",
        "            else:\n",
        "                print(f\"Não foi possível abrir a imagem: {caminho_arquivo}\")\n",
        "        else:\n",
        "            print(f\"Arquivo ignorado (não é uma imagem): {caminho_arquivo}\")\n",
        "\n",
        "# Salvar os DataFrames como CSV\n",
        "df1.to_csv('dataset1.csv', index=False)\n",
        "df2.to_csv('dataset2.csv', index=False)\n"
      ],
      "metadata": {
        "id": "OEZov1E7yM8D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Carregar o primeiro conjunto de dados\n",
        "df1 = pd.read_csv('dataset1.csv')\n",
        "\n",
        "# Separar os atributos das classes\n",
        "y = df1['label']\n",
        "X = df1.drop('label', axis=1)\n",
        "\n",
        "# Aplicar PCA com diferentes percentuais de variância explicada\n",
        "percentuais = [0.75, 0.85]\n",
        "for percentual in percentuais:\n",
        "    # Inicializar o objeto PCA com o percentual desejado\n",
        "    pca = PCA(n_components=percentual)\n",
        "\n",
        "    # Aplicar o PCA aos dados\n",
        "    X_reduced = pca.fit_transform(X)\n",
        "\n",
        "    # Criar um novo DataFrame com os atributos reduzidos\n",
        "    df_reduced = pd.DataFrame(X_reduced)\n",
        "    # Adicionar a coluna 'label' de volta ao DataFrame\n",
        "    df_reduced['label'] = y\n",
        "\n",
        "    # Salvar o DataFrame como CSV\n",
        "    df_reduced.to_csv(f'dataset1_pca_{percentual}.csv', index=False)"
      ],
      "metadata": {
        "id": "7lEjgl5ot15T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# modelo de CNN pré-treinado\n",
        "\n",
        "modelo = VGG16(weights='imagenet', include_top=False, pooling='avg')\n",
        "\n",
        "recursos_vgg_avg = []\n",
        "recursos_vgg_max = []\n",
        "rotulos = []\n",
        "\n",
        "# Para cada diretório na lista de diretórios\n",
        "for diretorio in diretorios:\n",
        "    # Para cada arquivo no diretório\n",
        "    for nome_arquivo in os.listdir(diretorio):\n",
        "        if nome_arquivo.endswith(('.png','.jpg', '.jpeg')):\n",
        "            caminho_arquivo = os.path.join(diretorio, nome_arquivo)\n",
        "            imagem = cv2.imread(caminho_arquivo)\n",
        "            if imagem is not None:\n",
        "                # Redimensionar a imagem para 224x224\n",
        "                imagem_redimensionada = cv2.resize(imagem, (224, 224))\n",
        "\n",
        "                # Converter a imagem para o formato esperado pelo VGG16\n",
        "                imagem_vgg = np.expand_dims(imagem_redimensionada, axis=0)\n",
        "                imagem_vgg = preprocess_input(imagem_vgg)\n",
        "\n",
        "                # Usar o VGG16 para extrair recursos\n",
        "                recursos_avg = modelo.predict(imagem_vgg)\n",
        "\n",
        "                # Mudar pooling para 'max' e extrair recursos novamente\n",
        "                modelo.pooling = 'max'\n",
        "                recursos_max = modelo.predict(imagem_vgg)\n",
        "\n",
        "                # Adicionar os recursos à lista\n",
        "                recursos_vgg_avg.append(recursos_avg.flatten())\n",
        "                recursos_vgg_max.append(recursos_max.flatten())\n",
        "\n",
        "                # Adicionar o rótulo à lista de rótulos\n",
        "                rotulo = 'cachorro' if 'basset_hound' in diretorio or 'york_terrier' in diretorio else 'gato'\n",
        "                rotulos.append(rotulo)\n",
        "\n",
        "# Criar DataFrames com os recursos extraídos e os rótulos correspondentes\n",
        "df_vgg_avg = pd.DataFrame(recursos_vgg_avg)\n",
        "df_vgg_max = pd.DataFrame(recursos_vgg_max)\n",
        "df_vgg_avg['label'] = rotulos\n",
        "df_vgg_max['label'] = rotulos\n",
        "\n",
        "df_vgg_avg.to_csv('dataset_vgg_avg.csv', index=False)\n",
        "df_vgg_max.to_csv('dataset_vgg_max.csv', index=False)"
      ],
      "metadata": {
        "id": "p0yjQ_UYdn1-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df1 = pd.read_csv('dataset1.csv')\n",
        "df2 = pd.read_csv('dataset2.csv')\n",
        "df_vgg_avg = pd.read_csv('dataset_vgg_avg.csv')\n",
        "df_vgg_max = pd.read_csv('dataset_vgg_max.csv')\n",
        "dataset1_pca = pd.read_csv('dataset1_pca_0.85.csv')\n",
        "dataset2_pca = pd.read_csv('dataset1_pca_0.75.csv')\n",
        "datasets = [df1, df2, df_vgg_avg, df_vgg_max, dataset1_pca, dataset2_pca]\n",
        "\n",
        "\n",
        "resultados = []\n",
        "\n",
        "# Para cada dataset\n",
        "for i, df in enumerate(datasets):\n",
        "    # Separar os atributos das classes\n",
        "    X = df.drop('label', axis=1)\n",
        "    y = df['label']\n",
        "\n",
        "    # Para cada valor de k de 1 a 10\n",
        "    for k in range(1, 11):\n",
        "\n",
        "        knn = KNeighborsClassifier(n_neighbors=k)\n",
        "\n",
        "        # 10-fold cross-validation\n",
        "        scores = cross_val_score(knn, X, y, cv=StratifiedKFold(10))\n",
        "        resultados.append(('Dataset'+str(i+1), '10-fold CV', k, scores.mean(), scores.std()))\n",
        "\n",
        "        # Treinos com a proporção 90/10, 80/20 e 70/30\n",
        "        for test_size in [0.1, 0.2, 0.3]:\n",
        "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size)\n",
        "            knn.fit(X_train, y_train)\n",
        "            y_pred = knn.predict(X_test)\n",
        "            acc = accuracy_score(y_test, y_pred)\n",
        "            resultados.append(('Dataset'+str(i+1), str(int(test_size*100))+'%', k, acc))\n",
        "\n",
        "print(resultados)\n",
        "# Converter os resultados em um DataFrame pandas para facilitar a visualização\n",
        "resultados_df = pd.DataFrame(resultados, columns=['Dataset', 'Método', 'k', 'Acurácia', 'Desvio padrão'])\n",
        "\n",
        "# Salvar o DataFrame como CSV\n",
        "resultados_df.to_csv('resultados.csv', index=False)"
      ],
      "metadata": {
        "id": "fCVfs7vNutQ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Lendo o arquivo CSV\n",
        "df = pd.read_csv('resultados.csv')\n",
        "\n",
        "# Criando uma figura e um conjunto de subtramas\n",
        "fig, ax =plt.subplots(1,1)\n",
        "\n",
        "# Escondendo os eixos\n",
        "ax.axis('tight')\n",
        "ax.axis('off')\n",
        "\n",
        "# Criando a tabela\n",
        "ax.table(cellText=df.values,\n",
        "         colLabels=df.columns,\n",
        "         cellLoc = 'center',\n",
        "         loc='center')\n",
        "\n",
        "# Salvando a figura\n",
        "plt.savefig('tabela.png')"
      ],
      "metadata": {
        "id": "akH4YlI_-yDP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Prática DT e NB\n",
        "df1 = pd.read_csv('dataset1.csv')\n",
        "df2 = pd.read_csv('dataset2.csv')\n",
        "df_vgg_avg = pd.read_csv('dataset_vgg_avg.csv')\n",
        "df_vgg_max = pd.read_csv('dataset_vgg_max.csv')\n",
        "dataset1_pca = pd.read_csv('dataset1_pca_0.85.csv')\n",
        "dataset2_pca = pd.read_csv('dataset1_pca_0.75.csv')\n",
        "datasets = [df1, df2, df_vgg_avg, df_vgg_max, dataset1_pca, dataset2_pca]\n",
        "\n",
        "# Lista de tuplas (nome, DataFrame)\n",
        "datasets = [('df1', df1), ('df2', df2), ('df_vgg_avg', df_vgg_avg), ('df_vgg_max', df_vgg_max), ('dataset1_pca', dataset1_pca), ('dataset2_pca', dataset2_pca)]\n",
        "\n",
        "# Lista de divisões de treinamento/teste\n",
        "splits = [(0.7, 0.3), (0.8, 0.2), (0.9, 0.1)]\n",
        "\n",
        "# Inicializar um DataFrame para armazenar os resultados\n",
        "results = pd.DataFrame(columns=['Base', 'Treinamento/Teste', 'md = 3', 'md = 4', 'md = 5', 'md = 6', 'md = 7', 'md = 8', 'md = 9', 'md = 10'])\n",
        "\n",
        "# Loop sobre os conjuntos de dados\n",
        "for name, dataset in datasets:\n",
        "    # Separar as características e o alvo\n",
        "    X = dataset.drop('label', axis=1)\n",
        "    y = dataset['label']\n",
        "\n",
        "    # Loop sobre as divisões de treinamento/teste\n",
        "    for train_size, test_size in splits:\n",
        "        # Dividir o conjunto de dados em treino e teste\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n",
        "\n",
        "        # Inicializar uma linha para este conjunto de dados e divisão\n",
        "        row = {'Base': name, 'Treinamento/Teste': f'{int(train_size * 100)}/{int(test_size * 100)}'}\n",
        "\n",
        "        # Loop sobre os valores de max_depth\n",
        "        for max_depth in range(3, 11):\n",
        "            # Inicializar a Árvore de Decisão\n",
        "            clf = DecisionTreeClassifier(max_depth=max_depth)\n",
        "\n",
        "            # Treinar o modelo e calcular a acurácia no conjunto de teste\n",
        "            clf.fit(X_train, y_train)\n",
        "            accuracy = clf.score(X_test, y_test)\n",
        "\n",
        "            # Adicionar a acurácia à linha\n",
        "            row[f'md = {max_depth}'] = accuracy\n",
        "\n",
        "        # Adicionar a linha ao DataFrame de resultados\n",
        "        results = results.append(row, ignore_index=True)\n",
        "\n",
        "    # Calcular a acurácia usando validação cruzada 10-fold para cada max_depth\n",
        "    row_cv = {'Base': name, 'Treinamento/Teste': '10-fold CV'}\n",
        "    for max_depth in range(3, 11):\n",
        "        clf_cv = DecisionTreeClassifier(max_depth=max_depth)\n",
        "        scores_cv = cross_val_score(clf_cv, X, y, cv=10)\n",
        "        row_cv[f'md = {max_depth}'] = scores_cv.mean()\n",
        "    results = results.append(row_cv, ignore_index=True)\n",
        "\n",
        "# Imprimir o DataFrame de resultados\n",
        "print(results)\n",
        "\n",
        "\n",
        "results.to_excel('results.xlsx')\n",
        "results.to_csv('results.csv')\n"
      ],
      "metadata": {
        "id": "hyEtfCmZlYdh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inicializar um DataFrame para armazenar os resultados\n",
        "results_nb = pd.DataFrame(columns=['Base', 'Treinamento/Teste', 'GaussianNB'])\n",
        "\n",
        "# Loop sobre os conjuntos de dados\n",
        "for name, dataset in datasets:\n",
        "    # Separar as características e o alvo\n",
        "    X = dataset.drop('label', axis=1)\n",
        "    y = dataset['label']\n",
        "\n",
        "    # Loop sobre as divisões de treinamento/teste\n",
        "    for train_size, test_size in splits:\n",
        "        # Dividir o conjunto de dados em treino e teste\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n",
        "\n",
        "        # Inicializar uma linha para este conjunto de dados e divisão\n",
        "        row = {'Base': name, 'Treinamento/Teste': f'{int(train_size * 100)}/{int(test_size * 100)}'}\n",
        "\n",
        "        # Inicializar o Gaussian Naive Bayes\n",
        "        gnb = GaussianNB()\n",
        "\n",
        "        # Treinar o modelo e calcular a acurácia no conjunto de teste\n",
        "        gnb.fit(X_train, y_train)\n",
        "        accuracy = gnb.score(X_test, y_test)\n",
        "\n",
        "        # Adicionar a acurácia à linha\n",
        "        row['GaussianNB'] = accuracy\n",
        "\n",
        "        # Adicionar a linha ao DataFrame de resultados\n",
        "        results_nb = results_nb.append(row, ignore_index=True)\n",
        "\n",
        "    # Calcular a acurácia usando validação cruzada 10-fold para GaussianNB\n",
        "    row_cv = {'Base': name, 'Treinamento/Teste': '10-fold CV'}\n",
        "    gnb_cv = GaussianNB()\n",
        "    scores_cv = cross_val_score(gnb_cv, X, y, cv=10)\n",
        "    row_cv['GaussianNB'] = scores_cv.mean()\n",
        "    results_nb = results_nb.append(row_cv, ignore_index=True)\n",
        "\n",
        "# Imprimir o DataFrame de resultados\n",
        "print(results_nb)\n",
        "results_nb.to_excel('results.xlsx')\n"
      ],
      "metadata": {
        "id": "6qUilQpUenMv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.model_selection import cross_val_score, train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Carregar os conjuntos de dados\n",
        "df1 = pd.read_csv('dataset1.csv')\n",
        "df2 = pd.read_csv('dataset2.csv')\n",
        "df_vgg_avg = pd.read_csv('dataset_vgg_avg.csv')\n",
        "df_vgg_max = pd.read_csv('dataset_vgg_max.csv')\n",
        "dataset1_pca = pd.read_csv('dataset1_pca_0.85.csv')\n",
        "dataset2_pca = pd.read_csv('dataset1_pca_0.75.csv')\n",
        "\n",
        "# Função para realizar experimentos com MLP\n",
        "def experiment_mlp(X, y, num_neurons, activation, max_iter, learning_rate):\n",
        "    # Dividir os dados em treinamento e teste (70/30, 80/20, 90/10)\n",
        "    acc_scores = {'70/30': [], '80/20': [], '90/10': [], '10-fold CV': []}\n",
        "\n",
        "    for test_size in [0.3, 0.2, 0.1]:\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n",
        "\n",
        "        # Inicializar o classificador MLP\n",
        "        mlp = MLPClassifier(hidden_layer_sizes=num_neurons, activation=activation, max_iter=max_iter,\n",
        "                            learning_rate_init=learning_rate, random_state=42)\n",
        "\n",
        "        # Treinar o modelo\n",
        "        mlp.fit(X_train, y_train)\n",
        "\n",
        "        # Avaliar a acurácia nos dados de teste\n",
        "        y_pred = mlp.predict(X_test)\n",
        "        acc_test = accuracy_score(y_test, y_pred)\n",
        "        acc_scores[f'{int((1-test_size)*100)}/{int(test_size*100)}'].append(acc_test)\n",
        "\n",
        "        # Realizar 10-fold cross-validation\n",
        "        acc_cv = cross_val_score(mlp, X, y, cv=10)\n",
        "        acc_scores['10-fold CV'].append(np.mean(acc_cv))\n",
        "\n",
        "    return acc_scores\n",
        "\n",
        "# Experimento 1: Fixar número de neurônios e variar funções de ativação\n",
        "num_neurons = int((X.shape[1] + 2) / 2)  # A = (número de atributos + número de classes) / 2\n",
        "activation_functions = ['identity', 'logistic', 'tanh', 'relu']\n",
        "\n",
        "for base, X, y in [('df1', df1.iloc[:, :-1], df1.iloc[:, -1]),\n",
        "                  ('df2', df2.iloc[:, :-1], df2.iloc[:, -1]),\n",
        "                  ('df_vgg_avg', df_vgg_avg.iloc[:, :-1], df_vgg_avg.iloc[:, -1]),\n",
        "                  ('df_vgg_max', df_vgg_max.iloc[:, :-1], df_vgg_max.iloc[:, -1]),\n",
        "                  ('dataset1_pca', dataset1_pca.iloc[:, :-1], dataset1_pca.iloc[:, -1]),\n",
        "                  ('dataset2_pca', dataset2_pca.iloc[:, :-1], dataset2_pca.iloc[:, -1])]:\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    for activation in activation_functions:\n",
        "        acc_scores = experiment_mlp(X, y, num_neurons, activation, 1000, 0.001)\n",
        "        plt.plot(acc_scores['70/30'], label=f'{activation} (70/30)')\n",
        "        plt.plot(acc_scores['80/20'], label=f'{activation} (80/20)')\n",
        "        plt.plot(acc_scores['90/10'], label=f'{activation} (90/10)')\n",
        "        plt.plot(acc_scores['10-fold CV'], label=f'{activation} (10-fold CV)')\n",
        "\n",
        "    plt.title(f'Experimento 1 - Base: {base}')\n",
        "    plt.xlabel('Experimento')\n",
        "    plt.ylabel('Acurácia')\n",
        "    plt.legend()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 409
        },
        "id": "PaGVnvlDYaH5",
        "outputId": "f7af2ca6-dfcd-41c3-9764-d8fff0e4eee3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-22154a916a91>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Carregar os conjuntos de dados\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mdf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dataset1.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mdf2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dataset2.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mdf_vgg_avg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dataset_vgg_avg.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1735\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1736\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1737\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    857\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'dataset1.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Função para realizar experimentos com MLP (Passo 2)\n",
        "def experiment_mlp_neurons(X, y, num_neurons_list, activation, max_iter, learning_rate):\n",
        "    acc_scores = {'Num Neurons': [], 'Accuracy': []}\n",
        "\n",
        "    for num_neurons in num_neurons_list:\n",
        "        for test_size in [0.3, 0.2, 0.1]:\n",
        "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n",
        "\n",
        "            mlp = MLPClassifier(hidden_layer_sizes=num_neurons, activation=activation, max_iter=max_iter,\n",
        "                                learning_rate_init=learning_rate, random_state=42)\n",
        "\n",
        "            mlp.fit(X_train, y_train)\n",
        "\n",
        "            y_pred = mlp.predict(X_test)\n",
        "            acc_test = accuracy_score(y_test, y_pred)\n",
        "            acc_scores['Num Neurons'].append(num_neurons)\n",
        "            acc_scores['Accuracy'].append(acc_test)\n",
        "\n",
        "    return acc_scores\n",
        "\n",
        "# Passo 2: Escolher a melhor função de ativação e variar o número de neurônios\n",
        "activation_functions = ['identity', 'logistic', 'tanh', 'relu']\n",
        "num_neurons_values = [2, 451, 900, 226, 675]\n",
        "\n",
        "for base, X, y in [('df1', df1.iloc[:, :-1], df1.iloc[:, -1]),\n",
        "                  ('df2', df2.iloc[:, :-1], df2.iloc[:, -1]),\n",
        "                  ('df_vgg_avg', df_vgg_avg.iloc[:, :-1], df_vgg_avg.iloc[:, -1]),\n",
        "                  ('df_vgg_max', df_vgg_max.iloc[:, :-1], df_vgg_max.iloc[:, -1]),\n",
        "                  ('dataset1_pca', dataset1_pca.iloc[:, :-1], dataset1_pca.iloc[:, -1]),\n",
        "                  ('dataset2_pca', dataset2_pca.iloc[:, :-1], dataset2_pca.iloc[:, -1])]:\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    for activation in activation_functions:\n",
        "        acc_scores = experiment_mlp_neurons(X, y, num_neurons_values, activation, 1000, 0.001)\n",
        "        plt.plot(acc_scores['Num Neurons'], acc_scores['Accuracy'], label=f'{activation}')\n",
        "\n",
        "    plt.title(f'Experimento 2 - Base: {base}')\n",
        "    plt.xlabel('Número de Neurônios')\n",
        "    plt.ylabel('Acurácia')\n",
        "    plt.legend()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "GQNItDGadB6d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Função para realizar experimentos com MLP (Passo 3)\n",
        "def experiment_mlp_iterations(X, y, num_neurons, activation, max_iter_list, learning_rate):\n",
        "    acc_scores = {'Num Iterations': [], 'Accuracy': []}\n",
        "\n",
        "    for max_iter in max_iter_list:\n",
        "        for test_size in [0.3, 0.2, 0.1]:\n",
        "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n",
        "\n",
        "            mlp = MLPClassifier(hidden_layer_sizes=num_neurons, activation=activation, max_iter=max_iter,\n",
        "                                learning_rate_init=learning_rate, random_state=42)\n",
        "\n",
        "            mlp.fit(X_train, y_train)\n",
        "\n",
        "            y_pred = mlp.predict(X_test)\n",
        "            acc_test = accuracy_score(y_test, y_pred)\n",
        "            acc_scores['Num Iterations'].append(max_iter)\n",
        "            acc_scores['Accuracy'].append(acc_test)\n",
        "\n",
        "    return acc_scores\n",
        "\n",
        "# Passo 3: Escolher o melhor número de neurônios e variar o número de iterações\n",
        "best_num_neurons = 451  # Substitua pelo melhor valor encontrado no Passo 2\n",
        "max_iterations_values = [100, 1000, 5000]\n",
        "\n",
        "for base, X, y in [('df1', df1.iloc[:, :-1], df1.iloc[:, -1]),\n",
        "                  ('df2', df2.iloc[:, :-1], df2.iloc[:, -1]),\n",
        "                  ('df_vgg_avg', df_vgg_avg.iloc[:, :-1], df_vgg_avg.iloc[:, -1]),\n",
        "                  ('df_vgg_max', df_vgg_max.iloc[:, :-1], df_vgg_max.iloc[:, -1]),\n",
        "                  ('dataset1_pca', dataset1_pca.iloc[:, :-1], dataset1_pca.iloc[:, -1]),\n",
        "                  ('dataset2_pca', dataset2_pca.iloc[:, :-1], dataset2_pca.iloc[:, -1])]:\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    acc_scores = experiment_mlp_iterations(X, y, best_num_neurons, 'relu', max_iterations_values, 0.001)\n",
        "    plt.plot(acc_scores['Num Iterations'], acc_scores['Accuracy'], label='relu')\n",
        "\n",
        "    plt.title(f'Experimento 3 - Base: {base}')\n",
        "    plt.xlabel('Número de Iterações')\n",
        "    plt.ylabel('Acurácia')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "h49UKWi1sYXW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Função para realizar experimentos com MLP (Passo 4)\n",
        "def experiment_mlp_learning_rate(X, y, num_neurons, activation, max_iter, learning_rate_list):\n",
        "    acc_scores = {'Learning Rate': [], 'Accuracy': []}\n",
        "\n",
        "    for learning_rate in learning_rate_list:\n",
        "        for test_size in [0.3, 0.2, 0.1]:\n",
        "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n",
        "\n",
        "            mlp = MLPClassifier(hidden_layer_sizes=num_neurons, activation=activation, max_iter=max_iter,\n",
        "                                learning_rate_init=learning_rate, random_state=42)\n",
        "\n",
        "            mlp.fit(X_train, y_train)\n",
        "\n",
        "            y_pred = mlp.predict(X_test)\n",
        "            acc_test = accuracy_score(y_test, y_pred)\n",
        "            acc_scores['Learning Rate'].append(learning_rate)\n",
        "            acc_scores['Accuracy'].append(acc_test)\n",
        "\n",
        "    return acc_scores\n",
        "\n",
        "# Passo 4: Escolher o melhor número de iterações e variar a taxa de aprendizado\n",
        "best_num_iterations = 1000  # Substitua pelo melhor valor encontrado no Passo 3\n",
        "learning_rate_values = [0.001, 0.01, 0.1]\n",
        "\n",
        "for base, X, y in [('df1', df1.iloc[:, :-1], df1.iloc[:, -1]),\n",
        "                  ('df2', df2.iloc[:, :-1], df2.iloc[:, -1]),\n",
        "                  ('df_vgg_avg', df_vgg_avg.iloc[:, :-1], df_vgg_avg.iloc[:, -1]),\n",
        "                  ('df_vgg_max', df_vgg_max.iloc[:, :-1], df_vgg_max.iloc[:, -1]),\n",
        "                  ('dataset1_pca', dataset1_pca.iloc[:, :-1], dataset1_pca.iloc[:, -1]),\n",
        "                  ('dataset2_pca', dataset2_p)]"
      ],
      "metadata": {
        "id": "rssBMUOUsfnT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Teste de Friedman para Gaussian NB\n",
        "# Cada linha é um dataset e cada coluna é um método (proporção de treinamento/teste)\n",
        "accs = [\n",
        "    [0.69, 0.70, 0.72, 0.78],  # HOG(16x16) 1764 Att\n",
        "    [0.69, 0.73, 0.72, 0.78],  # HOG(20x20) 900 Att\n",
        "    [0.65, 0.69, 0.67, 0.70],  # PCA HOG(16x16)\n",
        "    [0.99, 0.98, 0.97, 0.99],  # VGG 16 (Pooling max)\n",
        "    [0.99, 0.98, 0.97, 0.99]   # VGG 16 (Pooling avg)\n",
        "]\n",
        "\n",
        "stat, p = friedmanchisquare(*accs)\n",
        "\n",
        "print('Estatística de Friedman: %.3f' % stat)\n",
        "print('p-valor: %.3f' % p)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "siMUYhc_yD4b",
        "outputId": "0eb5a5f6-6aa9-45c1-c395-87a963af22cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Estatística de Friedman: 15.836\n",
            "p-valor: 0.003\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "accs_np = np.array([\n",
        "    [0.69, 0.70, 0.72, 0.78],  # HOG(16x16) 1764 Att\n",
        "    [0.69, 0.73, 0.72, 0.78],  # HOG(20x20) 900 Att\n",
        "    [0.65, 0.69, 0.67, 0.70],  # PCA HOG(16x16)\n",
        "    [0.99, 0.98, 0.97, 0.99],  # VGG 16 (Pooling max)\n",
        "    [0.99, 0.98, 0.97, 0.99]   # VGG 16 (Pooling avg)\n",
        "])\n",
        "\n",
        "# teste de Nemenyi\n",
        "result = sp.posthoc_nemenyi_friedman(accs_np.T)\n",
        "\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71OXLRGj3_DY",
        "outputId": "aefa9ff5-e59e-4361-9e6d-cdef2dd9c85d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "          0         1         2         3         4\n",
            "0  1.000000  0.900000  0.707384  0.316855  0.316855\n",
            "1  0.900000  1.000000  0.580482  0.449813  0.449813\n",
            "2  0.707384  0.580482  1.000000  0.015044  0.015044\n",
            "3  0.316855  0.449813  0.015044  1.000000  0.900000\n",
            "4  0.316855  0.449813  0.015044  0.900000  1.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = [\n",
        "    [0.67, 0.72, 0.68, 0.65, 0.65, 0.65, 0.62, 0.63],  # HOG(16x16) 1764 Att 10-fold CV\n",
        "    [0.67, 0.68, 0.61, 0.64, 0.64, 0.63, 0.64, 0.64],  # HOG(16x16) 1764 Att 70/30\n",
        "    [0.63, 0.60, 0.65, 0.63, 0.60, 0.61, 0.65, 0.65],  # HOG(16x16) 1764 Att 80/20\n",
        "    [0.65, 0.65, 0.66, 0.65, 0.67, 0.65, 0.63, 0.64],  # HOG(16x16) 1764 Att 90/10\n",
        "    [0.63, 0.66, 0.59, 0.59, 0.61, 0.56, 0.59, 0.59],  # HOG(20x20) 900 Att 10-fold CV\n",
        "    [0.60, 0.57, 0.63, 0.64, 0.61, 0.63, 0.60, 0.61],  # HOG(20x20) 900 Att 70/30\n",
        "    [0.69, 0.60, 0.65, 0.55, 0.61, 0.59, 0.53, 0.61],  # HOG(20x20) 900 Att 80/20\n",
        "    [0.67, 0.66, 0.64, 0.65, 0.65, 0.67, 0.67, 0.64],  # HOG(20x20) 900 Att 90/10\n",
        "    [0.65, 0.63, 0.64, 0.60, 0.60, 0.59, 0.58, 0.59],  # PCA HOG(16x16) 10-fold CV\n",
        "    [0.70, 0.69, 0.69, 0.67, 0.66, 0.62, 0.65, 0.67],  # PCA HOG(16x16) 70/30\n",
        "    [0.65, 0.65, 0.64, 0.65, 0.70, 0.66, 0.68, 0.63],  # PCA HOG(16x16) 80/20\n",
        "    [0.64, 0.63, 0.63, 0.61, 0.62, 0.63, 0.60, 0.61],  # PCA HOG(16x16) 90/10\n",
        "    [0.96, 0.95, 0.96, 0.96, 0.96, 0.96, 0.96, 0.95],  # VGG 16 (Pooling max) 10-fold CV\n",
        "    [0.95, 0.96, 0.96, 0.96, 0.96, 0.95, 0.96, 0.96],  # VGG 16 (Pooling max) 70/30\n",
        "    [0.95, 0.96, 0.96, 0.96, 0.96, 0.96, 0.96, 0.96],  # VGG 16 (Pooling max) 80/20\n",
        "    [0.97, 0.97, 0.96, 0.96, 0.96, 0.96, 0.97, 0.96],  # VGG 16 (Pooling max) 90/10\n",
        "    [0.97, 0.95, 0.96, 0.96, 0.96, 0.96, 0.96, 0.95],  # VGG 16 (Pooling avg) 10-fold CV\n",
        "    [0.96, 0.95, 0.96, 0.96, 0.94, 0.94, 0.96, 0.94],  # VGG 16 (Pooling avg) 70/30\n",
        "    [0.95, 0.96, 0.96, 0.96, 0.95, 0.95, 0.96, 0.96],  # VGG 16 (Pooling avg) 80/20\n",
        "    [0.97, 0.96, 0.96, 0.96, 0.96, 0.96, 0.96, 0.96]   # VGG 16 (Pooling avg) 90/10\n",
        "]\n",
        "\n",
        "# Realize o teste de Friedman\n",
        "stat, p = stats.friedmanchisquare(*data)\n",
        "\n",
        "print('Estatística de Friedman: %.3f' % stat)\n",
        "print('p-valor: %.3f' % p)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "il9HMVoG6Vq2",
        "outputId": "89db8aed-ec31-4459-a383-672392ab31d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Estatística de Friedman: 133.591\n",
            "p-valor: 0.000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "accs_np = np.array([\n",
        "    [0.67, 0.72, 0.68, 0.65, 0.65, 0.65, 0.62, 0.63],  # HOG(16x16) 1764 Att 10-fold CV\n",
        "    [0.67, 0.68, 0.61, 0.64, 0.64, 0.63, 0.64, 0.64],  # HOG(16x16) 1764 Att 70/30\n",
        "    [0.63, 0.60, 0.65, 0.63, 0.60, 0.61, 0.65, 0.65],  # HOG(16x16) 1764 Att 80/20\n",
        "    [0.65, 0.65, 0.66, 0.65, 0.67, 0.65, 0.63, 0.64],  # HOG(16x16) 1764 Att 90/10\n",
        "    [0.63, 0.66, 0.59, 0.59, 0.61, 0.56, 0.59, 0.59],  # HOG(20x20) 900 Att 10-fold CV\n",
        "    [0.60, 0.57, 0.63, 0.64, 0.61, 0.63, 0.60, 0.61],  # HOG(20x20) 900 Att 70/30\n",
        "    [0.69, 0.60, 0.65, 0.55, 0.61, 0.59, 0.53, 0.61],  # HOG(20x20) 900 Att 80/20\n",
        "    [0.67, 0.66, 0.64, 0.65, 0.65, 0.67, 0.67, 0.64],  # HOG(20x20) 900 Att 90/10\n",
        "    [0.65, 0.63, 0.64, 0.60, 0.60, 0.59, 0.58, 0.59],  # PCA HOG(16x16) 10-fold CV\n",
        "    [0.70, 0.69, 0.69, 0.67, 0.66, 0.62, 0.65, 0.67],  # PCA HOG(16x16) 70/30\n",
        "    [0.65, 0.65, 0.64, 0.65, 0.70, 0.66, 0.68, 0.63],  # PCA HOG(16x16) 80/20\n",
        "    [0.64, 0.63, 0.63, 0.61, 0.62, 0.63, 0.60, 0.61],  # PCA HOG(16x16) 90/10\n",
        "    [0.96, 0.95, 0.96, 0.96, 0.96, 0.96, 0.96, 0.95],  # VGG 16 (Pooling max) 10-fold CV\n",
        "    [0.95, 0.96, 0.96, 0.96, 0.96, 0.95, 0.96, 0.96],  # VGG 16 (Pooling max) 70/30\n",
        "    [0.95, 0.96, 0.96, 0.96, 0.96, 0.96, 0.96, 0.96],  # VGG 16 (Pooling max) 80/20\n",
        "    [0.97, 0.97, 0.96, 0.96, 0.96, 0.96, 0.97, 0.96],  # VGG 16 (Pooling max) 90/10\n",
        "    [0.97, 0.95, 0.96, 0.96, 0.96, 0.96, 0.96, 0.95],  # VGG 16 (Pooling avg) 10-fold CV\n",
        "    [0.96, 0.95, 0.96, 0.96, 0.94, 0.94, 0.96, 0.94],  # VGG 16 (Pooling avg) 70/30\n",
        "    [0.95, 0.96, 0.96, 0.96, 0.95, 0.95, 0.96, 0.96],  # VGG 16 (Pooling avg) 80/20\n",
        "    [0.97, 0.96, 0.96, 0.96, 0.96, 0.96, 0.96, 0.96]   # VGG 16 (Pooling avg) 90/10\n",
        "])\n",
        "\n",
        "# Realize o teste de Nemenyi\n",
        "result = sp.posthoc_nemenyi_friedman(accs_np.T)\n",
        "\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nYrWGvrf7UDi",
        "outputId": "11a2118c-7e69-44e4-9dc2-f964048c6e26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "          0         1         2         3         4         5         6   \\\n",
            "0   1.000000  0.900000  0.900000  0.900000  0.867949  0.900000  0.900000   \n",
            "1   0.900000  1.000000  0.900000  0.900000  0.900000  0.900000  0.900000   \n",
            "2   0.900000  0.900000  1.000000  0.900000  0.900000  0.900000  0.900000   \n",
            "3   0.900000  0.900000  0.900000  1.000000  0.900000  0.900000  0.900000   \n",
            "4   0.867949  0.900000  0.900000  0.900000  1.000000  0.900000  0.900000   \n",
            "5   0.900000  0.900000  0.900000  0.900000  0.900000  1.000000  0.900000   \n",
            "6   0.900000  0.900000  0.900000  0.900000  0.900000  0.900000  1.000000   \n",
            "7   0.900000  0.900000  0.900000  0.900000  0.826110  0.900000  0.900000   \n",
            "8   0.900000  0.900000  0.900000  0.900000  0.900000  0.900000  0.900000   \n",
            "9   0.900000  0.900000  0.900000  0.900000  0.547169  0.770322  0.854003   \n",
            "10  0.900000  0.900000  0.900000  0.900000  0.900000  0.900000  0.900000   \n",
            "11  0.900000  0.900000  0.900000  0.900000  0.900000  0.900000  0.900000   \n",
            "12  0.616901  0.231606  0.042795  0.519275  0.001278  0.005499  0.009154   \n",
            "13  0.589010  0.209315  0.037126  0.491221  0.001053  0.004617  0.007746   \n",
            "14  0.491221  0.143082  0.022047  0.381853  0.001000  0.002461  0.004229   \n",
            "15  0.178488  0.032087  0.003233  0.121376  0.001000  0.001000  0.001000   \n",
            "16  0.547169  0.178488  0.029795  0.446022  0.001000  0.003539  0.005997   \n",
            "17  0.900000  0.547169  0.169871  0.826110  0.009154  0.032087  0.049104   \n",
            "18  0.686638  0.293647  0.060482  0.589010  0.002049  0.008421  0.013781   \n",
            "19  0.334857  0.078971  0.009943  0.242804  0.001000  0.001000  0.001698   \n",
            "\n",
            "          7         8         9         10        11        12        13  \\\n",
            "0   0.900000  0.900000  0.900000  0.900000  0.900000  0.616901  0.589010   \n",
            "1   0.900000  0.900000  0.900000  0.900000  0.900000  0.231606  0.209315   \n",
            "2   0.900000  0.900000  0.900000  0.900000  0.900000  0.042795  0.037126   \n",
            "3   0.900000  0.900000  0.900000  0.900000  0.900000  0.519275  0.491221   \n",
            "4   0.826110  0.900000  0.547169  0.900000  0.900000  0.001278  0.001053   \n",
            "5   0.900000  0.900000  0.770322  0.900000  0.900000  0.005499  0.004617   \n",
            "6   0.900000  0.900000  0.854003  0.900000  0.900000  0.009154  0.007746   \n",
            "7   1.000000  0.900000  0.900000  0.900000  0.900000  0.658745  0.630849   \n",
            "8   0.900000  1.000000  0.644798  0.900000  0.900000  0.002461  0.002049   \n",
            "9   0.900000  0.644798  1.000000  0.900000  0.900000  0.900000  0.900000   \n",
            "10  0.900000  0.900000  0.900000  1.000000  0.900000  0.547169  0.519275   \n",
            "11  0.900000  0.900000  0.900000  0.900000  1.000000  0.014933  0.012713   \n",
            "12  0.658745  0.002461  0.900000  0.547169  0.014933  1.000000  0.900000   \n",
            "13  0.630849  0.002049  0.900000  0.519275  0.012713  0.900000  1.000000   \n",
            "14  0.533223  0.001053  0.812161  0.414146  0.007114  0.900000  0.900000   \n",
            "15  0.209315  0.001000  0.491221  0.134841  0.001000  0.900000  0.900000   \n",
            "16  0.589010  0.001544  0.867949  0.476605  0.009943  0.900000  0.900000   \n",
            "17  0.900000  0.016166  0.900000  0.854003  0.074012  0.900000  0.900000   \n",
            "18  0.728481  0.003870  0.900000  0.616901  0.022047  0.900000  0.900000   \n",
            "19  0.381853  0.001000  0.672691  0.267579  0.002955  0.900000  0.900000   \n",
            "\n",
            "          14        15        16        17        18        19  \n",
            "0   0.491221  0.178488  0.547169  0.900000  0.686638  0.334857  \n",
            "1   0.143082  0.032087  0.178488  0.547169  0.293647  0.078971  \n",
            "2   0.022047  0.003233  0.029795  0.169871  0.060482  0.009943  \n",
            "3   0.381853  0.121376  0.446022  0.826110  0.589010  0.242804  \n",
            "4   0.001000  0.001000  0.001000  0.009154  0.002049  0.001000  \n",
            "5   0.002461  0.001000  0.003539  0.032087  0.008421  0.001000  \n",
            "6   0.004229  0.001000  0.005997  0.049104  0.013781  0.001698  \n",
            "7   0.533223  0.209315  0.589010  0.900000  0.728481  0.381853  \n",
            "8   0.001053  0.001000  0.001544  0.016166  0.003870  0.001000  \n",
            "9   0.812161  0.491221  0.867949  0.900000  0.900000  0.672691  \n",
            "10  0.414146  0.134841  0.476605  0.854003  0.616901  0.267579  \n",
            "11  0.007114  0.001000  0.009943  0.074012  0.022047  0.002955  \n",
            "12  0.900000  0.900000  0.900000  0.900000  0.900000  0.900000  \n",
            "13  0.900000  0.900000  0.900000  0.900000  0.900000  0.900000  \n",
            "14  1.000000  0.900000  0.900000  0.900000  0.900000  0.900000  \n",
            "15  0.900000  1.000000  0.900000  0.900000  0.900000  0.900000  \n",
            "16  0.900000  0.900000  1.000000  0.900000  0.900000  0.900000  \n",
            "17  0.900000  0.900000  0.900000  1.000000  0.900000  0.900000  \n",
            "18  0.900000  0.900000  0.900000  0.900000  1.000000  0.900000  \n",
            "19  0.900000  0.900000  0.900000  0.900000  0.900000  1.000000  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Seus dados\n",
        "data = [\n",
        "    [0.67, 0.57, 0.61, 0.58, 0.60, 0.57, 0.59, 0.57, 0.58, 0.57],  # Dataset 1\n",
        "    [0.66, 0.61, 0.68, 0.54, 0.63, 0.58, 0.54, 0.46, 0.64, 0.56],  # Dataset 2\n",
        "    [0.62, 0.60, 0.56, 0.52, 0.53, 0.51, 0.61, 0.56, 0.65, 0.57],  # Dataset 3\n",
        "    [0.68, 0.59, 0.61, 0.52, 0.52, 0.54, 0.59, 0.54, 0.59, 0.58],  # Dataset 4\n",
        "    [0.69, 0.61, 0.66, 0.60, 0.64, 0.60, 0.61, 0.59, 0.63, 0.60],  # Dataset 5\n",
        "    [0.60, 0.71, 0.63, 0.59, 0.61, 0.58, 0.68, 0.63, 0.59, 0.51],  # Dataset 6\n",
        "    [0.72, 0.64, 0.64, 0.63, 0.62, 0.59, 0.59, 0.61, 0.60, 0.54],  # Dataset 7\n",
        "    [0.70, 0.61, 0.69, 0.53, 0.63, 0.63, 0.59, 0.59, 0.62, 0.58],  # Dataset 8\n",
        "    [0.68, 0.59, 0.64, 0.59, 0.62, 0.59, 0.61, 0.60, 0.60, 0.58],  # Dataset 9\n",
        "    [0.65, 0.60, 0.67, 0.60, 0.61, 0.51, 0.58, 0.48, 0.59, 0.58],  # Dataset 10\n",
        "    [0.67, 0.59, 0.54, 0.64, 0.67, 0.54, 0.63, 0.56, 0.63, 0.58],  # Dataset 11\n",
        "    [0.62, 0.61, 0.59, 0.59, 0.58, 0.59, 0.56, 0.63, 0.63, 0.63],  # Dataset 12\n",
        "    [0.98, 0.99, 0.98, 0.98, 0.98, 0.99, 0.98, 0.99, 0.98, 0.98],  # Dataset 13\n",
        "    [0.98, 0.98, 0.95, 1.00, 0.98, 0.96, 0.99, 0.99, 0.99, 1.00],  # Dataset 14\n",
        "    [1.00, 0.99, 0.98, 0.97, 0.98, 0.98, 0.97, 0.97, 1.00, 0.89],  # Dataset 15\n",
        "    [1.00, 0.99, 0.99, 0.98, 0.97, 0.98, 0.98, 0.98, 0.99, 0.98],  # Dataset 16\n",
        "    [0.98, 0.99, 0.98, 0.98, 0.98, 0.99, 0.98, 1.00, 0.98, 0.98],  # Dataset 17\n",
        "    [0.98, 0.96, 0.98, 0.99, 1.00, 0.99, 0.99, 0.99, 1.00, 1.00],  # Dataset 18\n",
        "    [0.98, 0.99, 0.97, 0.98, 0.98, 0.98, 0.97, 0.97, 0.99, 0.97],  # Dataset 19\n",
        "    [0.99, 0.99, 0.97, 0.99, 0.99, 1.00, 0.98, 0.97, 1.00, 0.97]   # Dataset 20\n",
        "]\n",
        "\n",
        "# Realize o teste de Friedman\n",
        "stat, p = stats.friedmanchisquare(*data)\n",
        "\n",
        "print('Estatística de Friedman: %.3f' % stat)\n",
        "print('p-valor: %.50f' % p)\n",
        "\n",
        "# Interpretação\n",
        "alpha = 0.05\n",
        "if p > alpha:\n",
        "    print('Mesmas distribuições (falha em rejeitar H0)')\n",
        "else:\n",
        "    print('Distribuições diferentes (rejeita H0)')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96h_Dprc_nWI",
        "outputId": "76dc4c8e-783c-4af1-f0eb-4e1606e41d0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Estatística de Friedman: 150.354\n",
            "p-valor: 0.00000000000000000000018729093976745308544713219836\n",
            "Distribuições diferentes (rejeita H0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Definir a matriz A e o vetor b\n",
        "A = np.array([-6,-2,1, 2, 3, 5, 7, 10, 11,20,36])\n",
        "b = np.array([36,4,1, 4, 9, 25, 49, 100, 121, 400, 1296])\n",
        "\n",
        "# Calcular a transposta de A\n",
        "A_T = A.transpose()\n",
        "\n",
        "# Calcular A^T A\n",
        "AtA = np.matmul(A_T, A)\n",
        "\n",
        "# Calcular a inversa de A^T A\n",
        "inv_AtA = np.linalg.inv(AtA)\n",
        "\n",
        "# Calcular A^T b\n",
        "ATb = np.matmul(A_T, b)\n",
        "\n",
        "# Calcular β\n",
        "beta = np.matmul(inv_AtA, ATb)\n",
        "\n",
        "# Imprimir o resultado\n",
        "print(beta)"
      ],
      "metadata": {
        "id": "Qryq40YS1b3W",
        "outputId": "2c55279d-e4e0-4254-fcad-bd0a4e0e1afa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "LinAlgError",
          "evalue": "0-dimensional array given. Array must be at least two-dimensional",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLinAlgError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-03cde669cff8>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Calcular a inversa de A^T A\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0minv_AtA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAtA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# Calcular A^T b\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/linalg/linalg.py\u001b[0m in \u001b[0;36minv\u001b[0;34m(a)\u001b[0m\n\u001b[1;32m    553\u001b[0m     \"\"\"\n\u001b[1;32m    554\u001b[0m     \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_makearray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m     \u001b[0m_assert_stacked_2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    556\u001b[0m     \u001b[0m_assert_stacked_square\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m     \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_commonType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/linalg/linalg.py\u001b[0m in \u001b[0;36m_assert_stacked_2d\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m             raise LinAlgError('%d-dimensional array given. Array must be '\n\u001b[0m\u001b[1;32m    207\u001b[0m                     'at least two-dimensional' % a.ndim)\n\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLinAlgError\u001b[0m: 0-dimensional array given. Array must be at least two-dimensional"
          ]
        }
      ]
    }
  ]
}